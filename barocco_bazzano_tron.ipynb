{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvdtBiPrGllf"
      },
      "source": [
        "# **Challenge 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcDOQnQMGwLR"
      },
      "source": [
        "Implement a **batched arbitrarily-size matrix multiplication** kernel.\n",
        "\n",
        "Let dimensions be identical for all matrix multiplications in the batch.<br>\n",
        "These being $m, k, n \\in \\mathbb{N}$.<br>\n",
        "While $batch \\in \\mathbb{N}$ is the batch size.\n",
        "\n",
        "You are provided as input the following matrices:<br>\n",
        "$N_0, N_1, N_2, ... N_{batch - 1} \\in \\mathbb{M}^{k \\times n}$<br>\n",
        "$M \\in \\mathbb{M}^{m \\times k}$\n",
        "\n",
        "You need to compute:<br>\n",
        "$P_0, P_1, P_2, ... P_{batch - 1} \\in \\mathbb{M}^{m \\times n}$\n",
        "\n",
        "Where $P_i = M \\otimes N_i$ for each $i \\in \\{0, ..., batch - 1\\}$.\n",
        "\n",
        "---\n",
        "\n",
        "A baseline reference implementation is given. Implement your version to replace it. Rely on the provided host-side function to check the correctness of results.\n",
        "\n",
        "To get a general performance metric rely on the profiler, specifically look for the `cuda_gpu_kern_sum` and try to minimize the `Total Time (ns)` of your kernel. Meanwhile, you may also want to improve `cuda_gpu_mem_time_sum`.\n",
        "\n",
        "Step one is beating the reference implementation, that should be easy, then you can use all tricks in the book to push it further.\n",
        "Anything goes, but if you use \"exotic\" tricks we want an explanation.\n",
        "In fact, submitting your work, be sure to fill out the [report](#report) with brief insights of what you did."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0olg8POKfsl"
      },
      "source": [
        "General rules and advice:\n",
        "- groups are of 3 members at most, that should, for as much as possible, equally contribute to the project.\n",
        "- you automagically get ~3 points in the 1st part of the exam, taking the form of 1-2 questions that you will be allowed to skip.\n",
        "- deadline is 1 week (as of this writing, you will need to submit your work before october 23 at 23:59).\n",
        "- submissions are to be made on WeBeep, where you need to upload a downloaded copy of this notebook (.ipynb file); for groups of multiple people, it's enough for one member to submit the file in the assignment, other members shall simply write their group's name in their submission, we will then infer groups from what you write in the report section.\n",
        "- your code needs to work here on Colab with the T4 runtime.\n",
        "- do not alter code sections delimited by \"===\"s in the final submission.\n",
        "- we will change around matrix sizes arbitrarily while evaluating your work, so make sure to cover all edge cases and take care that your code is scalable (e.g. execution time grows as expected when doubling all dimensions).\n",
        "- you can get the maximum grade just by using what was discussed during lectures or is present in the glossary shown during exercise sessions; still, if you wanna have \"more fun\" this guide is your best friend https://docs.nvidia.com/cuda/cuda-c-best-practices-guide.\n",
        "- a piece of code that works is better than a supposedly faster piece of code that doesn't, so don't go overboard, but be ambitious.\n",
        "- use LLMs (ChatGPT and friends) responsibly; the purpose of this challenge is for you to get your hands dirty and build up confidence in writing parallel code through trial and error. Having an LLM write your code may get you the challenge's points (unless it's so blatant that we notice), but won't lead you to learn anything and the next time you see some parallel code your mind goes blank. If you wack your head at the problem instead, and solve it, the solution will stick in the back of your mind for a long time. Similarly, if despite pushing yourself you can't find \"that damn bug\", then asking an LLM is fine, so long as you tried first by yourself and just say \"ahhhhhh, so that what it was!\" upon having the LLM help you out. Long story short, AI is fine so long as it's a tool you **learn from** and **not** one you **blindly lean on**.\n",
        "\n",
        "If you need help or anything, please drop us an email:\n",
        "- Dr. M. Ronzani: marco.ronzani@polimi.it\n",
        "- Prof. F. Ferrandi: fabrizio.ferrandi@polimi.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHGYx97cJqaE"
      },
      "source": [
        "## **Colab Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWrw0NgzGlMq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITAYKD7MGcmH"
      },
      "outputs": [],
      "source": [
        "!mkdir /home/cuda\n",
        "%cd /home/cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9rbKG58Jyw6"
      },
      "source": [
        "## **Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ys4rptyJ5EJ"
      },
      "outputs": [],
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "#define TILE_SIZE 16\n",
        "#define THREAD_TILE_M 4\n",
        "#define THREAD_TILE_N 4\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  __shared__ float tileM[TILE_SIZE * THREAD_TILE_M][TILE_SIZE];\n",
        "  __shared__ float tileN[TILE_SIZE][TILE_SIZE * THREAD_TILE_N];\n",
        "  int tx = threadIdx.x;\n",
        "  int ty = threadIdx.y;\n",
        "  int b = blockIdx.z;\n",
        "\n",
        "  int baseRow = (blockIdx.y * TILE_SIZE + ty) * THREAD_TILE_M;\n",
        "  int baseCol = (blockIdx.x * TILE_SIZE + tx) * THREAD_TILE_N;\n",
        "  int numTiles = (k + TILE_SIZE - 1) / TILE_SIZE;\n",
        "\n",
        "  float values[THREAD_TILE_M][THREAD_TILE_N] = {0.0f};\n",
        "\n",
        "  for (int t = 0; t < numTiles; t++) {\n",
        "    int kOffset = t * TILE_SIZE;\n",
        "\n",
        "    for (int i = 0; i < THREAD_TILE_M; i++) {\n",
        "      int row = baseRow + i;\n",
        "      int col = kOffset + tx;\n",
        "      tileM[ty * THREAD_TILE_M + i][tx] = (row < m && col < k) ? M[row * k + col] : 0.0f;\n",
        "    }\n",
        "\n",
        "    for (int j = 0; j < THREAD_TILE_N; j++) {\n",
        "      int row = kOffset + ty;\n",
        "      int col = baseCol + j;\n",
        "      tileN[ty][tx * THREAD_TILE_N + j] = (row < k && col < n) ? N[b * (k * n) + row * n + col] : 0.0f;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int kk = 0; kk < TILE_SIZE; kk++) {\n",
        "      for (int i = 0; i < THREAD_TILE_M; i++) {\n",
        "        float a = tileM[ty * THREAD_TILE_M + i][kk];\n",
        "        for (int j = 0; j < THREAD_TILE_N; j++) {\n",
        "          float b = tileN[kk][tx * THREAD_TILE_N + j];\n",
        "          values[i][j] += a * b;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  for (int i = 0; i < THREAD_TILE_M; i++) {\n",
        "    for (int j = 0; j < THREAD_TILE_N; j++) {\n",
        "      int row = baseRow + i;\n",
        "      int col = baseCol + j;\n",
        "      if (row < m && col < n) {\n",
        "        P[b * (m * n) + row * n + col] = values[i][j];\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, (sizeM + sizeN + sizeP) * sizeof(float)));\n",
        "  // gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  // gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "  N_d = M_d + sizeM;\n",
        "  P_d = N_d + sizeN;\n",
        "\n",
        "  cudaStream_t stream;\n",
        "  gpuErrchk(cudaStreamCreate(&stream));\n",
        "\n",
        "  gpuErrchk(cudaMemcpyAsync(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice, stream));\n",
        "  gpuErrchk(cudaMemcpyAsync(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice, stream));\n",
        "  gpuErrchk(cudaMemcpyAsync(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice, stream));\n",
        "\n",
        "  dim3 blockSize(TILE_SIZE, TILE_SIZE, 1);\n",
        "  dim3 numBlocks(\n",
        "    (n + TILE_SIZE*THREAD_TILE_M - 1) / (TILE_SIZE*THREAD_TILE_M),\n",
        "    (m + TILE_SIZE*THREAD_TILE_N - 1) /(TILE_SIZE*THREAD_TILE_N),\n",
        "    batch);\n",
        "\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpyAsync(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost, stream));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4eaFcePJ_8r"
      },
      "source": [
        "## **Compile, Run, Profile**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXV_BmFYKM2q"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKs3tgz6KDYD"
      },
      "outputs": [],
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlgkX-SPZhqf"
      },
      "outputs": [],
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBn4aD0gKRq7"
      },
      "source": [
        "Profile:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRLC6R5AKRTs"
      },
      "outputs": [],
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edxNOq-8PCdP"
      },
      "source": [
        "<a name=\"report\"></a>\n",
        "## **Brief Report**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5COx5mKMPF7m"
      },
      "source": [
        "**You must fill in this section!!**\n",
        "\n",
        "Group information:\n",
        "- member-1: Tommaso, Tron, 10828422\n",
        "- member-2: Francesco, Bazzano, 10828115\n",
        "- member-3: Giorgio, Barocco, 10842713\n",
        "- cuBLASTed <br><img src=\"https://i.ibb.co/vCMzb0wZ/Chat-GPT-Image-23-ott-2025-13-19-35.png\" alt=\"+0.005 points at the exam if you know the reference ^-^\" width=\"120\" border=\"0\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRKJGx2MR7LQ"
      },
      "source": [
        "Bullet points describing what you did with a short motivation:\n",
        "- We use shared-memory tiling for M and N, letting threads load contiguous blocks, reuse data to cut global transactions to the computer memory, improving effective bandwidth and arithmetic intensity per fetched element.\n",
        "\n",
        "- The batch dimension is mapped to grid.z so independent matrices run concurrently; this preserves coalesced accesses within M and N tiles, simplifies indexing with per-batch base pointers.\n",
        "\n",
        "- Each thread calculates a small THREAD_TILE_M x THREAD_TILE_N block of the output, accumulating in registers to maximize reuse of loaded operands. This increases efficiency and keeps the GPU cores busier during execution. We tested different values and obtained the best results with 4x4.\n",
        "\n",
        "- We experimented with a prefetching strategy to hide memory access latency, but the performance turned out to be worse than expected. In this context, we observed a tradeoff between kernel execution time and data transfer time, and we ultimately chose to optimize the configuration to minimize the kernel's computation time.\n",
        "\n",
        "- Allocating one large buffer and using async copies in a dedicated stream is an attempt to reduce memory overhead.\n",
        "\n",
        "*Note: possibly less than 8 entries of ~32 words each. More isn't necessarily better if nobody will read it.*\n",
        "\n",
        "*Note: the subject is \"the main things you came up with to improve the kernel\".*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iHGYx97cJqaE"
      ],
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
